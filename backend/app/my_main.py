# æ•´åˆå¾Œç«¯API + ç¶²é 
import os
import time
import logging
from pathlib import Path
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import google.genai as genai
from google.genai.types import GenerateContentConfig
from openai import AsyncOpenAI
from dotenv import load_dotenv
import httpx

from backend.app.services.rag_core import setup_rag_system, get_rag_chain, stream_answer, DMS_stream_answer, build_prompt, retrieve_context
from backend.app.utils.stream_utils import stream_content

# ==================== åˆå§‹åŒ–è¨­å®š ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="LLM Chatbot Web")

# ADD
try:
    from backend.app.services.st_code_parser_backend import add_st_parser_routes
    add_st_parser_routes(app)
except ImportError as e:
    logger.error(f"ST è§£æå™¨åŒ¯å…¥å¤±æ•—: {e}")

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ¨¡æ¿è¨­å®š:ç¢ºä¿ templates è³‡æ–™å¤¾å­˜åœ¨
# templates_dir = Path(__file__).parent / "templates"
# templates_dir.mkdir(exist_ok=True)
# templates = Jinja2Templates(directory=str(templates_dir))

# ==================== å…¨å±€è®Šæ•¸ ====================
# custom_system_prompt = os.getenv("CUSTOM_SYSTEM_PROMPT", "You are a helpful AI assistant.")
prompt_parts = [
    value for key, value in sorted(os.environ.items())
    if key.startswith("CUSTOM_PROMPT_")
]
custom_system_prompt = (
    " ".join(p.strip() for p in prompt_parts if p)
    or "You are a helpful AI assistant that replies in Markdown."
)

# åˆå§‹åŒ– Gemini
gemini_client = None
try:
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if gemini_api_key:
        gemini_client = genai.Client(api_key=gemini_api_key)
        logger.info("âœ“ Gemini å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
    else:
        logger.warning("âš  GEMINI_API_KEY æœªè¨­å®š")
except Exception as e:
    logger.error(f"âœ— Gemini åˆå§‹åŒ–å¤±æ•—: {e}")

# åˆå§‹åŒ– OpenRouter
openrouter_client = None
try:
    openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
    if openrouter_api_key:
        openrouter_http_client = httpx.AsyncClient(trust_env=False)
        openrouter_client = AsyncOpenAI(
            api_key=openrouter_api_key,
            base_url="https://openrouter.ai/api/v1",
            http_client=openrouter_http_client
        )
        logger.info("âœ“ OpenRouter å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
    else:
        logger.warning("âš  OPENROUTER_API_KEY æœªè¨­å®š")
except Exception as e:
    logger.error(f"âœ— OpenRouter åˆå§‹åŒ–å¤±æ•—: {e}")

# åˆå§‹åŒ– DMS
dms_client = None
try:
    dms_api_key = os.getenv("DMS_API_KEY")
    if dms_api_key:
        dms_http_client = httpx.AsyncClient(trust_env=False)
        dms_client = AsyncOpenAI(
            api_key=dms_api_key,
            base_url="https://llmgateway.deltaww.com/v1/",
            http_client=dms_http_client
        )
        logger.info("âœ“ DMS å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"âœ— DMS åˆå§‹åŒ–å¤±æ•—: {e}")

# ==================== è«‹æ±‚æ¨¡å‹ ====================
class ChatRequest(BaseModel):
    message: str
    provider: str = "gemini"
    model: str = "gemini-2.0-flash"
    temperature: float = 0.7
    max_tokens: int = 2000

# ==================== ç¶²é è·¯ç”± ====================
# @app.get("/", response_class=HTMLResponse)
# async def index(request: Request):
#     """ä¸»é é¢"""
#     return templates.TemplateResponse("index.html", {"request": request})

# ==================== API ç«¯é» ====================

# 1. Gemini åŸç”Ÿä¸²æµï¼ˆä¸ä½¿ç”¨ RAGï¼‰
@app.get("/gemini_native_stream")
async def gemini_native_stream(question: str):
    """ç›´æ¥ç™¼é€å•é¡Œçµ¦ Gemini"""
    async def event_generator():
        if not gemini_client:
            yield "data: [éŒ¯èª¤] Gemini æœå‹™æœªåˆå§‹åŒ–\n\n"
            return
            
        try:
            logger.info(f"Gemini åŸç”Ÿå•é¡Œ: {question}")
           
            config = GenerateContentConfig(
                max_output_tokens=2000,
                temperature=0.7
            )
           
            contents = [custom_system_prompt, question]
            response_stream = await gemini_client.aio.models.generate_content_stream(
                model="gemini-2.0-flash",
                contents=contents,
                config=config
            )

            async for line in stream_content(response_stream, "gemini"):
                yield line

        except Exception as e:
            logger.error(f"Gemini åŸç”Ÿä¸²æµéŒ¯èª¤: {e}")
            yield f"data: [éŒ¯èª¤] {str(e)}\n\n"
   
    return StreamingResponse(event_generator(), media_type="text/event-stream")

# ç¬¬2æ®µå¥½åƒæœ¬ä¾†å°±æ²’æœ‰ç”¨RAGï¼Œæ‰€ä»¥æˆ‘çš„ç¬¬2æ®µæ‰æ˜¯æœ¬ä¾†çš„Gemini_native
# FIXME: é‡å¯«ä¸€æ®µæœ‰åŠ å…¥RAGçš„ 
# åˆå§‹åŒ– RAG ç³»çµ±
RAG_ENABLED = False
try:
    logger.info("ğŸ”§ åˆå§‹åŒ– RAG æª¢ç´¢ç³»çµ±...")
    PDF_PATH = "D:/Build_RAG_Locally/DIADesigner-ST-CODE.pdf"
    setup_rag_system(PDF_PATH, force_reload=False)
    RAG_ENABLED = True
    logger.info("âœ“ RAG æª¢ç´¢ç³»çµ±åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"âœ— RAG åˆå§‹åŒ–å¤±æ•—: {e}")
    
# 2. Gemini ä¸²æµï¼ˆæ”¯æ´è‡ªè¨‚æç¤ºï¼‰
@app.get("/gemini_stream")
async def gemini_stream(question: str, request: Request, use_rag: bool = True):
    """Gemini ä¸²æµï¼ˆæ”¯æ´ RAGï¼‰"""
    async def event_generator():
        if not gemini_client:
            yield "data: [éŒ¯èª¤] Gemini æœå‹™æœªåˆå§‹åŒ–\n\n"
            return
       
        try:
            logger.info(f"Gemini å•é¡Œ (RAG={use_rag and RAG_ENABLED}): {question}")
           
            # RAG æª¢ç´¢
            if use_rag and RAG_ENABLED:
                try:
                    ctx, sources = retrieve_context(question, k=5, max_chars=8000)
                    sources_label = "\n".join([f"[S{i+1}] {src}" for i, src in enumerate(sources)])
                    rag_prompt = build_prompt(question, ctx, sources_label)
                    contents = [custom_system_prompt, rag_prompt]
                    logger.info(f"âœ“ ä½¿ç”¨ RAGï¼Œæª¢ç´¢åˆ° {len(sources)} å€‹æ–‡ä»¶")
                except Exception as e:
                    logger.warning(f"RAG æª¢ç´¢å¤±æ•—: {e}ï¼Œä½¿ç”¨åŸå§‹å•é¡Œ")
                    contents = [custom_system_prompt, question]
            else:
                contents = [custom_system_prompt, question]
            
            # Gemini ä¸²æµ
            config = GenerateContentConfig(max_output_tokens=2000, temperature=0.7)
            response_stream = await gemini_client.aio.models.generate_content_stream(
                model="gemini-2.0-flash",
                contents=contents,
                config=config
            )

            async for line in stream_content(response_stream, "gemini"):
                yield line

        except Exception as e:
            logger.error(f"Gemini ä¸²æµéŒ¯èª¤: {e}")
            yield f"data: [éŒ¯èª¤] {str(e)}\n\n"
   
    return StreamingResponse(event_generator(), media_type="text/event-stream")

# 3. OpenRouter ä¸²æµ
@app.get("/openrouter_stream")
async def openrouter_stream(question: str):
    """OpenRouter ä¸²æµ"""
    async def event_generator():
        if not openrouter_client:
            yield "data: [éŒ¯èª¤] OpenRouter æœå‹™æœªåˆå§‹åŒ–\n\n"
            return
        
        try:
            logger.info(f"OpenRouter å•é¡Œ: {question}")
           
            response = await openrouter_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": os.getenv("REACT_APP_API_SERVER"),
                    "X-Title": "LLM Chatbot",
                },
                model="qwen/qwen3-235b-a22b:free",
                messages=[
                    {"role": "system", "content": custom_system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=0.7,
                max_tokens=2000,
                stream=True
            )

            async for line in stream_content(response, "openrouter"):
                yield line

        except Exception as e:
            logger.error(f"OpenRouter ä¸²æµéŒ¯èª¤: {e}")
            yield f"data: [éŒ¯èª¤] {str(e)}\n\n"
   
    return StreamingResponse(event_generator(), media_type="text/event-stream")

# 4. DMS ä¸²æµ
@app.get("/dms_stream")
async def dms_stream(question: str):
    """DMS ä¸²æµ"""
    async def event_generator():
        if not dms_client:
            yield "data: [éŒ¯èª¤] DMS æœå‹™æœªåˆå§‹åŒ–\n\n"
            return
        
        try:
            logger.info(f"DMS å•é¡Œ: {question}")
           
            response = await dms_client.chat.completions.create(
                model="openai/Qwen/Qwen3-Next-80B-A3B-Instruct",
                messages=[
                    {"role": "system", "content": custom_system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=0.7,
                max_tokens=8192,
                presence_penalty=1.5,
                stream=True
            )

            async for line in stream_content(response, "dms"):
                yield line

        except Exception as e:
            logger.error(f"DMS ä¸²æµéŒ¯èª¤: {e}")
            yield f"data: [éŒ¯èª¤] {str(e)}\n\n"
   
    return StreamingResponse(event_generator(), media_type="text/event-stream")

# 5. çµ±ä¸€ POST ç«¯é»ï¼ˆæ”¯æ´ JSON è«‹æ±‚ï¼‰
@app.post("/chat")
async def chat(request: ChatRequest):
    """çµ±ä¸€èŠå¤©ç«¯é»ï¼ˆPOSTï¼‰"""
    try:
        provider = request.provider.lower()
        
        if provider == "gemini":
            async def generate_gemini():
                if not gemini_client:
                    yield "éŒ¯èª¤: Gemini æœªåˆå§‹åŒ–".encode("utf-8")
                    return
               
                try:
                    config = GenerateContentConfig(
                        max_output_tokens=request.max_tokens,
                        temperature=request.temperature
                    )
                    contents = [custom_system_prompt, request.message]
                    response_stream = await gemini_client.aio.models.generate_content_stream(
                        model=request.model,
                        contents=contents,
                        config=config
                    )
                   
                    async for chunk in response_stream:
                        if chunk.text:
                            yield chunk.text.encode("utf-8")
                except Exception as e:
                    yield f"éŒ¯èª¤: {str(e)}".encode("utf-8")
           
            return StreamingResponse(generate_gemini(), media_type="text/plain")
       
        elif provider == "openrouter":
            async def generate_openrouter():
                if not openrouter_client:
                    yield "éŒ¯èª¤: OpenRouter æœªåˆå§‹åŒ–".encode("utf-8")
                    return
               
                try:
                    response = await openrouter_client.chat.completions.create(
                        extra_headers={
                            "HTTP-Referer": "http://localhost:8888",
                            "X-Title": "LLM Chatbot",
                        },
                        model=request.model,
                        messages=[
                            {"role": "system", "content": custom_system_prompt},
                            {"role": "user", "content": request.message}
                        ],
                        temperature=request.temperature,
                        max_tokens=request.max_tokens,
                        stream=True
                    )
                   
                    async for chunk in response:
                        if chunk.choices[0].delta.content:
                            yield chunk.choices[0].delta.content.encode("utf-8")
                except Exception as e:
                    yield f"éŒ¯èª¤: {str(e)}".encode("utf-8")
           
            return StreamingResponse(generate_openrouter(), media_type="text/plain")
       
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æ´çš„æä¾›è€…: {provider}")
   
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ==================== å·¥å…·ç«¯é» ====================
@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "ok",
        "providers": {
            "gemini": gemini_client is not None,
            "openrouter": openrouter_client is not None,
            "dms": dms_client is not None
        }
    }

@app.post("/reload_prompt")
async def reload_prompt():
    """é‡æ–°è¼‰å…¥ç³»çµ±æç¤º"""
    global custom_system_prompt
    load_dotenv(override=True)
    # custom_system_prompt = os.getenv("CUSTOM_SYSTEM_PROMPT", "You are a helpful AI assistant.")
    prompt_parts = [
        value for key, value in sorted(os.environ.items())
        if key.startswith("CUSTOM_PROMPT_")
    ]
    custom_system_prompt = (
        " ".join(p.strip() for p in prompt_parts if p)
        or "You are a helpful AI assistant that replies in Markdown."
    )
    return {
        "status": "å·²é‡æ–°è¼‰å…¥",
        "new_prompt": custom_system_prompt
    }

# ==================== å•Ÿå‹• ====================
if __name__ == "__main__":
    import uvicorn
    print("\n" + "=" * 60)
    print("  LLM Chatbot ä¼ºæœå™¨å•Ÿå‹•ä¸­...")
    print("=" * 60)
    print(f"  ç¶²é ä»‹é¢: http://localhost:8001")
    print(f"  API æ–‡æª”: http://localhost:8001/docs")
    print("=" * 60 + "\n")
   
    uvicorn.run(app, host="0.0.0.0", port=8888, log_level="info")

